{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN-final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kwdjQDt6Btq"
      },
      "source": [
        "Imports and Reading in Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsJu5iHg37mo"
      },
      "source": [
        "# imports and reading in datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import io\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YyqlAeU4epF"
      },
      "source": [
        "X_train = pd.read_csv(io.StringIO(uploaded['X_train.csv'].decode('utf-8')))\n",
        "X_train.drop('country', axis = 1, inplace = True) # drop the string column\n",
        "Y_train_5 = pd.read_csv(io.StringIO(uploaded['Y5_train.csv'].decode('utf-8')))\n",
        "Y_train_25 = pd.read_csv(io.StringIO(uploaded['Y25_train.csv'].decode('utf-8')))\n",
        "Y_train_50 = pd.read_csv(io.StringIO(uploaded['Y50_train.csv'].decode('utf-8')))\n",
        "\n",
        "# scale data\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "X_train = min_max_scaler.fit_transform(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTxr9hO945Ms"
      },
      "source": [
        "X_test = pd.read_csv(io.StringIO(uploaded['X_test.csv'].decode('utf-8')))\n",
        "X_test.drop('country', axis = 1, inplace = True) # drop the string column\n",
        "Y_test_5 = pd.read_csv(io.StringIO(uploaded['Y5_test.csv'].decode('utf-8')))\n",
        "Y_test_25 = pd.read_csv(io.StringIO(uploaded['Y25_test.csv'].decode('utf-8')))\n",
        "Y_test_50 = pd.read_csv(io.StringIO(uploaded['Y50_test.csv'].decode('utf-8')))\n",
        "\n",
        "# scale data\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "X_test = min_max_scaler.fit_transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ4wPBkv47KO"
      },
      "source": [
        "Building and Training the NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvHTDnnX49Kq"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meUEECRP4_uJ"
      },
      "source": [
        "model = Sequential([\n",
        "    Dense(32, activation='relu', input_shape=(58,)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='linear'),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiFjgyqc5FD2"
      },
      "source": [
        "model.compile(optimizer='adam',loss='mean_squared_error', metrics=['mse'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXNeNVYi5GtL"
      },
      "source": [
        "X_train_float = np.asarray(X_train).astype(np.float32)\n",
        "Y_train_float = np.asarray(Y_train_5).astype(np.float32)\n",
        "X_val_float = np.asarray(X_test).astype(np.float32)\n",
        "Y_val_float = np.asarray(Y_test_5).astype(np.float32)\n",
        "hist = model.fit(X_train_float, Y_train_float,\n",
        "          batch_size=32, epochs=100, validation_data=(X_val_float, Y_val_float))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkLgfCwJ5O5h"
      },
      "source": [
        "X_test_float = np.asarray(X_test).astype(np.float32)\n",
        "Y_test_float = np.asarray(X_test).astype(np.float32)\n",
        "model.evaluate(X_test_float, Y_test_float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-UXQrjC5WGY"
      },
      "source": [
        " Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGW7wPkp5XdU"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXcxAGMS5ZCL"
      },
      "source": [
        "# training loss and validation loss\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ5ARA855d_h"
      },
      "source": [
        "Overfitted Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rfg8DDZb5e9r"
      },
      "source": [
        "model_2 = Sequential([\n",
        "    Dense(1000, activation='relu', input_shape=(58,)),\n",
        "    Dense(1000, activation='relu'),\n",
        "    Dense(1000, activation='relu'),\n",
        "    Dense(1000, activation='relu'),\n",
        "    Dense(1, activation='linear'),\n",
        "])\n",
        "model_2.compile(optimizer='adam',\n",
        "              loss='mean_squared_error')\n",
        "hist_2 = model_2.fit(X_train_float, Y_train_float,\n",
        "          batch_size=32, epochs=100, validation_data=(X_val_float, Y_val_float))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G6tpGxo5nGz"
      },
      "source": [
        "plt.plot(hist_2.history['loss'])\n",
        "plt.plot(hist_2.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSurarHv5ppC"
      },
      "source": [
        "Incorporating Regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akpfVhZY5rpD"
      },
      "source": [
        "from keras.layers import Dropout\n",
        "from keras import regularizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upkdMctg5tOg"
      },
      "source": [
        "model_3 = Sequential([\n",
        "    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01), input_shape=(58,)),\n",
        "    Dropout(0.3),\n",
        "    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    Dropout(0.3),\n",
        "    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    Dropout(0.3),\n",
        "    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='linear', kernel_regularizer=regularizers.l2(0.01)),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PySMxm2d5u9X"
      },
      "source": [
        "model_3.compile(optimizer='adam',\n",
        "              loss='mean_squared_error')\n",
        "hist_3 = model_3.fit(X_train_float, Y_train_float,\n",
        "          batch_size=32, epochs=100, validation_data=(X_val_float, Y_val_float))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPRqMr1v5zoM"
      },
      "source": [
        "plt.plot(hist_3.history['loss'])\n",
        "plt.plot(hist_3.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A69Tr73N52Km"
      },
      "source": [
        "Printing out Prediction Values on the test dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ1HXO2f55Cc"
      },
      "source": [
        "ynew = model.predict(X_test_float)\n",
        "for i in range(len(ynew)):\n",
        "  print(ynew[i])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}